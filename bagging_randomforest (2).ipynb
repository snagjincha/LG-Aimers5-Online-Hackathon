{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffbb840d-f106-4864-84c9-006ec0a79aad",
   "metadata": {},
   "source": [
    "# **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "0e384a94-fea6-4f09-8528-ffaf64728267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.impute\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1259007-72af-4d72-a0fb-e0cce5a0e0e1",
   "metadata": {},
   "source": [
    "# **2. train 데이터 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "13e11b8a-02aa-4c7b-9f1d-3fffd989996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803cc13f-4479-4250-9fd8-75748a08ddeb",
   "metadata": {},
   "source": [
    "## **2-A. train 불필요한 컬럼삭제**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "37407d35-8360-4ba7-9b6d-4decfc0124af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변동성이 없는 데이터 제거 (분산 0)\n",
    "columns_to_drop1 = [col for col in train.columns if train[col].nunique() == 1]\n",
    "\n",
    "# 결측치 비율이 90% 이상인 열 제거\n",
    "missing_ratio = train.isnull().mean()\n",
    "threshold = 0.9\n",
    "columns_to_drop2 = missing_ratio[missing_ratio >= threshold].index.tolist()\n",
    "\n",
    "# 중복되는 컬럼 삭제 확실\n",
    "remove1=['Model.Suffix_AutoClave', 'Model.Suffix_Fill1', 'Model.Suffix_Fill2']\n",
    "remove2=['Workorder_AutoClave', 'Workorder_Fill1', 'Workorder_Fill2']\n",
    "\n",
    "remove_columns = columns_to_drop1 + columns_to_drop2 + remove1 + remove2\n",
    "\n",
    "train = train.drop(columns=remove_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9e930e-9420-4610-97fe-a5dd6b1ae2db",
   "metadata": {},
   "source": [
    "## **2-B. 결측치 처리**\n",
    "- 랜덤포레스트를 사용할 것이니 -999로 대체해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "6edc0b0a-f431-4e5a-a354-baf061d02cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 있는 컬럼 찾기\n",
    "missing = train.isnull().sum()\n",
    "missing_index = [i for i,v in enumerate(missing) if v>0] # 16,86,116번 컬럼에 결측치 발견\n",
    "missing_index\n",
    "missing_columns = [train.columns[missing_index[0]],train.columns[missing_index[1]],train.columns[missing_index[2]]]\n",
    "\n",
    "# OK를 NaN으로 대체\n",
    "for i in range(3): \n",
    "    train[missing_columns[i]] = train[missing_columns[i]].replace('OK', np.nan)\n",
    "\n",
    "# NaN값을 -999로 대체\n",
    "for i in range(3): \n",
    "    train[missing_columns[i]] = train[missing_columns[i]].replace(np.nan, -999)\n",
    "\n",
    "# 값들을 float형으로 변환\n",
    "for col in missing_columns:\n",
    "    train[col] = train[col].astype(float) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64e4c6-ac6a-409c-bb0d-672dec85e6b7",
   "metadata": {},
   "source": [
    "## **2-C. 상관관계 1.0 이상 컬럼 삭제**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "bf0a056c-0e71-485b-93f8-8c8d55e54b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중공선성 일으키는 컬럼 삭제 (상관관계 1.0이상, 연속형 변수)\n",
    "corr_matrix = train.select_dtypes(include=[float, int]).corr().abs()\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >= 1.0)]\n",
    "\n",
    "train = train.drop(columns=to_drop)\n",
    "train = train.drop('CURE END POSITION X Collect Result_Dam',axis=1) # 여기서는 상관관계를 계산하지 않은 상관관계가 1인 다른 범주형 변수가 존재함. 그것과의 충돌을 막기 위해 이 컬럼도 삭제함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d7b98-021d-417d-8a6d-801257642d9c",
   "metadata": {},
   "source": [
    "## **2-D. 피처 엔지니어링**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "dcd7cfe6-c9c0-4195-9bb9-48026519d7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) num이 다르면 항상 비정상이 나오는 패턴 확인\n",
    "train['Dam_num'] = [l.split()[-1][-1] for l in train['Equipment_Dam']]\n",
    "train['Fill1_num'] = [l.split()[-1][-1] for l in train['Equipment_Fill1']]\n",
    "train['Fill2_num'] = [l.split()[-1][-1] for l in train['Equipment_Fill2']]\n",
    "\n",
    "train['Num_Diff'] = ((train['Dam_num'] != train['Fill1_num']) | \n",
    "                     (train['Dam_num'] != train['Fill2_num']) |\n",
    "                     (train['Fill1_num'] != train['Fill2_num']))\n",
    "train = train.drop(['Dam_num','Fill1_num','Fill2_num'],axis=1)\n",
    "\n",
    "# (2) collect result의 값이 다르면 항상 비정상이 나오는 패턴 확인\n",
    "train['is_abnormal_pattern'] = (train['Production Qty Collect Result_Fill1'] != train['Production Qty Collect Result_Dam']) | \\\n",
    "                               (train['Production Qty Collect Result_Fill1'] != train['Production Qty Collect Result_Fill2']) | \\\n",
    "                               (train['Production Qty Collect Result_Dam'] != train['Production Qty Collect Result_Fill2'])\n",
    "\n",
    "# (3) Receip No이 값이 다르면 항상 비정상이 나오는 패턴 확인\n",
    "train['is_abnormal_pattern2'] = (train['Receip No Collect Result_Dam'] != train['Receip No Collect Result_Fill1']) | \\\n",
    "                               (train['Receip No Collect Result_Dam'] != train['Receip No Collect Result_Fill1']) | \\\n",
    "                               (train['Receip No Collect Result_Fill1'] != train['Receip No Collect Result_Fill2'])\n",
    "\n",
    "# (4) Resin의 출사 속도와 시간의 곱을 이용해서 얼마나 레진이 나왔는지 새로운 컬럼 제작 -> 레진이 적게 나올 수록 비정상 비율이 높음\n",
    "train['multi'] = train['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * train['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "\n",
    "# (5) 온도와 압력의 곱이 특정 값일 때 항상 비정상\n",
    "train['temp_pressure_interaction'] = (train['Chamber Temp. Collect Result_AutoClave'] * train['1st Pressure Collect Result_AutoClave'])\n",
    "train['abnormal_interaction'] = train['temp_pressure_interaction'].apply(lambda x: False if x == 9.952 or x == 10.404 else True)\n",
    "\n",
    "# (6) 특정 컬럼에서의 비정상 패턴 발견\n",
    "train['Is_Abnormal_WorkMode'] = (train['WorkMode Collect Result_Fill2'] == 3)\n",
    "train['Pressure_Below_75'] = (train['3rd Pressure Unit Time_AutoClave'] < 75)\n",
    "train['Suffix_Dam_AJX75334503'] = (train['Model.Suffix_Dam'] == 'AJX75334503')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d40478d-52b8-4f4d-9785-5e1e57104b38",
   "metadata": {},
   "source": [
    "## **2-E. int로 되어있지만 사실은 범주형 -> object형으로 바꾸는 과정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "fe58817d-83c4-40a7-a1a6-b4efc1877672",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['WorkMode Collect Result_Fill2','WorkMode Collect Result_Fill1']] = train[['WorkMode Collect Result_Fill2','WorkMode Collect Result_Fill1']].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32c9ee-779c-4b2b-abf9-3be70440c274",
   "metadata": {},
   "source": [
    "## **2-F. 랜덤포레스트 학습을 위한 원핫인코딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e9ce9483-fdd1-46cf-b934-3cc59a1b296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1. 범주형 자료만 뽑기 (target 제외)\n",
    "categorical_cols = train.select_dtypes(include='object').columns.drop('target')\n",
    "\n",
    "# step2. 원핫 인코딩\n",
    "onehot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoded_data = onehot_encoder.fit_transform(train[categorical_cols])\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=onehot_encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# step3. 나머지 데이터프레임과 concat\n",
    "# 원핫 인코딩된 데이터와 target 열을 분리\n",
    "train_non_categorical = train.drop(categorical_cols,axis=1)\n",
    "train_target = train_non_categorical.pop('target')\n",
    "\n",
    "# step3-1. target 열을 0,1으로 레이블링\n",
    "dct = {'Normal':0 , 'AbNormal':1}\n",
    "train_target = train_target.map(dct)\n",
    "\n",
    "# step4. 원핫 인코딩된 데이터프레임과 나머지 데이터프레임을 합치기\n",
    "train = pd.concat([train_non_categorical, encoded_df, train_target], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4805a6-74db-479d-aa7e-f3e7fe976601",
   "metadata": {},
   "source": [
    "## **2-G. 특정 조건, 특정 값에서 항상 비정상을 보이는 것들 추가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "21e020b4-ed0b-4777-86a0-d5271eb6a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    ('Chamber Temp. Unit Time_AutoClave', 183),\n",
    "    ('Chamber Temp. Unit Time_AutoClave', 180),\n",
    "    ('Workorder_Dam_3KPXX094-0001', 1.0),\n",
    "    ('Workorder_Dam_4CPXX084-0001', 1.0)\n",
    "]\n",
    "\n",
    "# 각 조건에 대해 새로운 열 추가\n",
    "for col, value in conditions:\n",
    "    condition_name = f'Condition_{col}_{value}'\n",
    "    train[condition_name] = (train[col] == value).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531add9a-3bae-4ef1-b735-481ef8fb156c",
   "metadata": {},
   "source": [
    "# **3. test 데이터 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "73a56c9f-3374-4941-a7ce-ee3160d9c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba947a3d-3074-4eec-9d9d-311ab1d32352",
   "metadata": {},
   "source": [
    "## **3-A. test 불필요한 컬럼삭제**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "f9ca64d9-71d5-40df-91e1-34fc0a857dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변동성이 없는 데이터 제거\n",
    "columns_to_drop1 = [col for col in test.columns if test[col].nunique() == 1]\n",
    "\n",
    "# 결측치 비율이 90% 이상인 열 제거\n",
    "missing_ratio = test.isnull().mean()\n",
    "threshold = 0.9\n",
    "columns_to_drop2 = missing_ratio[missing_ratio >= threshold].index.tolist()\n",
    "\n",
    "# 중복되는 컬럼 삭제 확실\n",
    "remove1=['Model.Suffix_AutoClave', 'Model.Suffix_Fill1', 'Model.Suffix_Fill2']\n",
    "remove2=['Workorder_AutoClave', 'Workorder_Fill1', 'Workorder_Fill2']\n",
    "\n",
    "remove_columns = columns_to_drop1 + columns_to_drop2 + remove1 + remove2\n",
    "\n",
    "test = test.drop(columns=remove_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c9da0-0708-483e-901e-dab0a88f1cff",
   "metadata": {},
   "source": [
    "## **3-B. 결측치 처리**\n",
    "- 랜덤포레스트를 사용할 것이니 -999로 대체해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "9f50211f-c4d4-4e9c-85a4-c9b9258e393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 있는 컬럼 찾기\n",
    "missing = test.isnull().sum()\n",
    "missing_index = [i for i,v in enumerate(missing) if v>0] # 16,86,116번 컬럼에 결측치 발견\n",
    "missing_index\n",
    "missing_columns = [test.columns[missing_index[0]],test.columns[missing_index[1]],test.columns[missing_index[2]]]\n",
    "\n",
    "# OK를 NaN으로 대체\n",
    "for i in range(3): \n",
    "    test[missing_columns[i]] = test[missing_columns[i]].replace('OK', np.nan)\n",
    "\n",
    "# NaN값을 -999로 대체\n",
    "for i in range(3): \n",
    "    test[missing_columns[i]] = test[missing_columns[i]].replace(np.nan, -999)\n",
    "\n",
    "# 값들을 float형으로 변환\n",
    "for col in missing_columns:\n",
    "    test[col] = test[col].astype(float) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755023d-adf5-4804-8043-ef9dd5204645",
   "metadata": {},
   "source": [
    "## **3-C. 상관관계 1.0 이상 컬럼 삭제**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30460f3a-054c-4d7c-9c08-de08a9094e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중공선성 일으키는 컬럼 삭제 (상관관계 1.0이상, 연속형 변수)\n",
    "corr_matrix = test.select_dtypes(include=[float, int]).corr().abs()\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >= 1.0)]\n",
    "\n",
    "test = test.drop(columns=to_drop)\n",
    "test = test.drop('CURE END POSITION X Collect Result_Dam',axis=1) # 여기서는 상관관계를 계산하지 않은 상관관계가 1인 다른 범주형 변수가 존재함. 그것과의 충돌을 막기 위해 이 컬럼도 삭제함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535158f-ae29-4400-b2fb-7c268e838204",
   "metadata": {},
   "source": [
    "## **3-D. 피처 엔지니어링**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74016a0a-5efb-4434-bc26-3b964be1e3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) num이 다르면 항상 비정상이 나오는 패턴 확인\n",
    "test['Dam_num'] = [l.split()[-1][-1] for l in test['Equipment_Dam']]\n",
    "test['Fill1_num'] = [l.split()[-1][-1] for l in test['Equipment_Fill1']]\n",
    "test['Fill2_num'] = [l.split()[-1][-1] for l in test['Equipment_Fill2']]\n",
    "\n",
    "test['Num_Diff'] = ((test['Dam_num'] != test['Fill1_num']) | \n",
    "                     (test['Dam_num'] != test['Fill2_num']) |\n",
    "                     (test['Fill1_num'] != test['Fill2_num']))\n",
    "test = test.drop(['Dam_num','Fill1_num','Fill2_num'],axis=1)\n",
    "\n",
    "# (2) collect result의 값이 다르면 항상 비정상이 나오는 패턴 확인\n",
    "test['is_abnormal_pattern'] = (test['Production Qty Collect Result_Fill1'] != test['Production Qty Collect Result_Dam']) | \\\n",
    "                               (test['Production Qty Collect Result_Fill1'] != test['Production Qty Collect Result_Fill2']) | \\\n",
    "                               (test['Production Qty Collect Result_Dam'] != test['Production Qty Collect Result_Fill2'])\n",
    "\n",
    "# (3) Receip No이 값이 다르면 항상 비정상이 나오는 패턴 확인\n",
    "test['is_abnormal_pattern2'] = (test['Receip No Collect Result_Dam'] != test['Receip No Collect Result_Fill1']) | \\\n",
    "                               (test['Receip No Collect Result_Dam'] != test['Receip No Collect Result_Fill1']) | \\\n",
    "                               (test['Receip No Collect Result_Fill1'] != test['Receip No Collect Result_Fill2'])\n",
    "\n",
    "# (4) Resin의 출사 속도와 시간의 곱을 이용해서 얼마나 레진이 나왔는지 새로운 컬럼 제작 -> 레진이 적게 나올 수록 비정상 비율이 높음\n",
    "test['multi'] = test['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * test['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "\n",
    "# (5) 온도와 압력의 곱이 특정 값일 때 항상 비정상\n",
    "test['temp_pressure_interaction'] = (test['Chamber Temp. Collect Result_AutoClave'] * test['1st Pressure Collect Result_AutoClave'])\n",
    "test['abnormal_interaction'] = test['temp_pressure_interaction'].apply(lambda x: False if x == 9.952 or x == 10.404 else True)\n",
    "\n",
    "# (6) 특정 컬럼에서의 비정상 패턴 발견\n",
    "test['Is_Abnormal_WorkMode'] = (test['WorkMode Collect Result_Fill2'] == 3)\n",
    "test['Pressure_Below_75'] = (test['3rd Pressure Unit Time_AutoClave'] < 75)\n",
    "test['Suffix_Dam_AJX75334503'] = (test['Model.Suffix_Dam'] == 'AJX75334503')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b2b64-b974-4af0-b6a0-06665db18df7",
   "metadata": {},
   "source": [
    "## **3-E. int로 되어있지만 사실은 범주형 -> object형으로 바꾸는 과정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d2f60e-b953-400a-9635-d0257831090d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['WorkMode Collect Result_Fill2','WorkMode Collect Result_Fill1']] = test[['WorkMode Collect Result_Fill2','WorkMode Collect Result_Fill1']].astype('object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e49b8-468a-4fc6-84b8-fff038b1b377",
   "metadata": {},
   "source": [
    "## **3-F. 랜덤포레스트 학습을 위한 원핫인코딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5094a9-298f-459d-bc04-abf53c1e8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop('Set ID',axis=1)\n",
    "\n",
    "# step1. 범주형 자료만 뽑기 (target 제외)\n",
    "categorical_cols = test.select_dtypes(include='object').columns\n",
    "\n",
    "# step2. 원핫 인코딩\n",
    "onehot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "encoded_data = onehot_encoder.fit_transform(test[categorical_cols])\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=onehot_encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# step3. 나머지 데이터프레임과 concat\n",
    "# 원핫 인코딩된 데이터와 target 열을 분리\n",
    "test_non_categorical = test.drop(categorical_cols,axis=1)\n",
    "\n",
    "# step4. 원핫 인코딩된 데이터프레임과 나머지 데이터프레임을 합치기\n",
    "test = pd.concat([test_non_categorical, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57cdba1-3c4d-4a75-96ff-2ab38659e736",
   "metadata": {},
   "source": [
    "## **3-G. 특정 조건, 특정 값에서 항상 비정상을 보이는 것들 추가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014cfbf1-5493-4d6a-bf1c-d11728ba9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    ('Chamber Temp. Unit Time_AutoClave', 183),\n",
    "    ('Chamber Temp. Unit Time_AutoClave', 180),\n",
    "    ('Workorder_Dam_3KPXX094-0001', 1.0),\n",
    "    ('Workorder_Dam_4CPXX084-0001', 1.0)\n",
    "]\n",
    "# 각 조건에 대해 새로운 열 추가\n",
    "for col, value in conditions:\n",
    "    condition_name = f'Condition_{col}_{value}'\n",
    "    test[condition_name] = (test[col] == value).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad40ca4-19e8-4a01-a3f0-13cf99ddf08d",
   "metadata": {},
   "source": [
    "# **4. train과 test에서 중복되는 열만 선택**\n",
    "- train과 test의 일관성 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "c7baafb4-4810-483e-a369-8c2093a1a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 데이터프레임의 열 이름 추출\n",
    "columns_train = train.columns\n",
    "columns_test = test.columns\n",
    "target = train['target']\n",
    "\n",
    "# 중복 열 찾기\n",
    "common_columns = columns_train.intersection(columns_test)\n",
    "\n",
    "# 중복 열로 새로운 데이터프레임 생성\n",
    "train = pd.concat([train[common_columns],target],axis=1)\n",
    "test = test[common_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3135a4-46d4-4821-aafd-6f239fd0a82a",
   "metadata": {},
   "source": [
    "# **5. Hyper Parameters 수정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c1c448-815b-4e9e-8bf7-4c7c40e6cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 데이터 준비\n",
    "X = train.drop('target', axis=1)\n",
    "y = train['target']\n",
    "\n",
    "# 하이퍼파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'criterion': ['entropy','gini'],\n",
    "    'n_estimators': [400,500],  # 트리의 수\n",
    "    'max_depth': [None], # 트리의 최대 깊이\n",
    "    'min_samples_split': [2], # 내부 노드를 분할하는 데 필요한 최소 샘플 수\n",
    "    'min_samples_leaf': [1]   # 리프 노드의 최소 샘플 수\n",
    "}\n",
    "\n",
    "# RandomForestClassifier 객체 생성\n",
    "rf = RandomForestClassifier(random_state=110)\n",
    "\n",
    "# GridSearchCV 객체 생성\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='f1', n_jobs=-1, verbose=2)\n",
    "\n",
    "# 모델 학습\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# 최적의 하이퍼파라미터와 성능 추출\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best F1 score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4826d4d4-77a5-414d-aade-fd98630a46b6",
   "metadata": {},
   "source": [
    "# **6. 학습**\n",
    "- Imbalance 데이터이기 때문에 언더샘플링 진행\n",
    "- **하지만 언더샘플링 특성상 다수의 클래스의 정보손실이 생긴다. 그래서 언더 샘플링을 무작위로 100번정도 진행하여 100개의 학습된 랜덤포레스트를 만든 후 평균을 취하는 방식으로 모델을 합친다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "eae413ee-0160-484e-862a-d4e498e25213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.2891\n"
     ]
    }
   ],
   "source": [
    "X = train.drop('target',axis=1)\n",
    "y = train['target']\n",
    "\n",
    "# 데이터 분할 (train/validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# RandomForestClassifier의 하이퍼파라미터 설정\n",
    "rf_params = {\n",
    "    'criterion': 'gini',\n",
    "    'n_estimators': 400,  # 트리의 수\n",
    "    'max_depth': None, # 트리의 최대 깊이\n",
    "    'min_samples_split': 2, # 내부 노드를 분할하는 데 필요한 최소 샘플 수\n",
    "    'min_samples_leaf': 1,   # 리프 노드의 최소 샘플 수\n",
    "    'random_state': 110\n",
    "}\n",
    "\n",
    "# 모델 설정\n",
    "n_undersampling_iterations = 100\n",
    "\n",
    "# 개별 RandomForestClassifier 모델을 학습시키기 위한 리스트\n",
    "rf_models = []\n",
    "\n",
    "# 무작위 언더샘플링 및 모델 학습\n",
    "for _ in range(n_undersampling_iterations):\n",
    "    # 비정상 데이터만 샘플링\n",
    "    df_abnormal = X_train[X_train[\"target\"] == 1]\n",
    "    df_normal = X_train[X_train[\"target\"] == 0]\n",
    "    \n",
    "    # 비정상 데이터의 수만큼 샘플링\n",
    "    n_abnormal_sample = int(len(df_abnormal))\n",
    "    df_abnormal_sampled = df_abnormal.sample(n=n_abnormal_sample, replace=False)\n",
    "    \n",
    "    # 정상 데이터도 비정상 데이터 수에 맞게 샘플링\n",
    "    df_normal_sampled = df_normal.sample(n=n_abnormal_sample, replace=False)\n",
    "    \n",
    "    # 샘플링된 데이터 결합\n",
    "    df_balanced = pd.concat([df_abnormal_sampled, df_normal_sampled])\n",
    "    \n",
    "    # 훈련 데이터와 타겟 분리\n",
    "    X_balanced = df_balanced.drop('target', axis=1)\n",
    "    y_balanced = df_balanced['target']\n",
    "    \n",
    "    # 랜덤포레스트 모델 학습\n",
    "    rf_model = RandomForestClassifier(**rf_params)\n",
    "    rf_model.fit(X_balanced, y_balanced)\n",
    "    rf_models.append(rf_model)\n",
    "\n",
    "# 예측 수행 및 성능 평가\n",
    "def predict_with_multiple_models(models, X):\n",
    "    \"\"\" 여러 모델의 예측을 평균하여 최종 예측을 결정합니다. \"\"\"\n",
    "    predictions = np.zeros((X.shape[0], len(models)))\n",
    "    for i, model in enumerate(models):\n",
    "        predictions[:, i] = model.predict_proba(X)[:, 1]\n",
    "    return (np.mean(predictions, axis=1) > 0.5).astype(int)\n",
    "\n",
    "# 검증 데이터에 대해 예측 수행\n",
    "y_pred_val = predict_with_multiple_models(rf_models, X_val)\n",
    "\n",
    "# f1 score 계산\n",
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "print(f\"Validation F1 Score: {f1_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b13a05-f4b4-4053-947d-963e6c802706",
   "metadata": {},
   "source": [
    "# **7. 제출**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "ab9e129a-5dc3-4f32-8d07-2134c83a5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {0:'Normal',1:'AbNormal'}\n",
    "prediction = pd.Series(predict_with_multiple_models(rf_models, test)).map(dct)\n",
    "\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = prediction\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"연구7_배깅100_2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
